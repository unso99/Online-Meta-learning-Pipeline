{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9MdztalHrpnI",
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1683271115715,
     "user": {
      "displayName": "마형규",
      "userId": "13640479509153940851"
     },
     "user_tz": -540
    },
    "id": "9MdztalHrpnI"
   },
   "outputs": [],
   "source": [
    "# library\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "\n",
    "\n",
    "# load Neural Network Model Library => condition 3 of assignment\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ec770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "train_path = \"../../dataset/cifar10/train/\"\n",
    "test_path = \"../../dataset/cifar10/test/\"\n",
    "\n",
    "batch_size_num = 100\n",
    "epochs_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f173b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator with rescaler(1./255) -> condition 2 of assignment\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.1)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# make a loading data flow from path. It generates data at each batch sizes -> condition 1 of assignment\n",
    "train_generator = train_datagen.flow_from_directory(train_path,\n",
    "                                                    classes=sorted(os.listdir(train_path)),\n",
    "                                                    batch_size = batch_size_num,\n",
    "                                                    target_size = (256, 256),\n",
    "                                                    subset=\"training\",\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(train_path, \n",
    "                                                    classes=sorted(os.listdir(train_path)), \n",
    "                                                    batch_size = batch_size_num, \n",
    "                                                    target_size=(256, 256), \n",
    "                                                    subset=\"validation\",\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_path, \n",
    "                                                  classes=sorted(os.listdir(test_path)),\n",
    "                                                  batch_size = batch_size_num,\n",
    "                                                  target_size = (256, 256),\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "print()\n",
    "# check shape of data shape\n",
    "print(\"check shape of data shape\")\n",
    "for x_data, class_data in train_generator:\n",
    "    print(f\"input data shape from train_generator: {x_data.shape}\")\n",
    "    print(f\"class data shape from train_generator: {class_data.shape}\")\n",
    "    break\n",
    "\n",
    "for x_data, class_data in valid_generator:\n",
    "    print(f\"input data shape from valid_generator: {x_data.shape}\")\n",
    "    print(f\"class data shape from valid_generator: {class_data.shape}\")\n",
    "    break\n",
    "    \n",
    "for x_data, class_data in test_generator:\n",
    "    print(f\"input data shape from test_generator: {x_data.shape}\")\n",
    "    print(f\"class data shape from test_generator: {class_data.shape}\")\n",
    "    break    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22508ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set neural model architecture (transform AlexNet architecture)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 첫 번째 컨볼루션 레이어\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 두 번째 컨볼루션 레이어\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 완전 연결 레이어\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "\n",
    "# 출력 레이어\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WWFRavhArNa0",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1683271123809,
     "user": {
      "displayName": "마형규",
      "userId": "13640479509153940851"
     },
     "user_tz": -540
    },
    "id": "WWFRavhArNa0"
   },
   "outputs": [],
   "source": [
    "# train with fit_generator method\n",
    "# steps_per_epoch => same with iterations(train_data_size//batch_size) in model.fit\n",
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.n//valid_generator.batch_size,\n",
    "    shuffle=True,\n",
    "    epochs=epochs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7m9p1-MErKVU",
   "metadata": {
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1683271124264,
     "user": {
      "displayName": "마형규",
      "userId": "13640479509153940851"
     },
     "user_tz": -540
    },
    "id": "7m9p1-MErKVU"
   },
   "outputs": [],
   "source": [
    "# accuracy plot\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "# plt.savefig(f'./result_plot/batch_{batch_size_num}_epoch_{epochs_num}_acc_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cMAOQY3xrHFu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 863478,
     "status": "ok",
     "timestamp": 1683272277526,
     "user": {
      "displayName": "마형규",
      "userId": "13640479509153940851"
     },
     "user_tz": -540
    },
    "id": "cMAOQY3xrHFu",
    "outputId": "be37730b-8f71-42f8-d261-316b39ee3678"
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "acc = history.history['loss']\n",
    "val_acc = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, label=\"Training loss\")\n",
    "plt.plot(epochs, val_acc, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 3)\n",
    "# plt.savefig(f'./result_plot/batch_{batch_size_num}_epoch_{epochs_num}_loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ab475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"-- Evaluate --\")\n",
    "scores = model.evaluate_generator(test_generator, steps=test_generator.n//test_generator.batch_size)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../pretrained_model_repo/model_LeNet_cifar10.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
